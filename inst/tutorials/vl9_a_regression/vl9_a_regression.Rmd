---
title: "Regression"
output:
  learnr::tutorial:
    language: de
    css: css/boxes.css
    fig_caption: no
runtime: shiny_prerendered
bibliography: ref.json
link-citations: TRUE
description: Einführung in die einfache lineare Regression.
resource_files:
- css/boxes.css
---

```{r setup, include=FALSE}
library(learnr)
library(ggplot2)
library(shiny)
knitr::opts_chunk$set(echo = FALSE)

if(require(ggbrace)) library(ggbrace) else {devtools::install_github("NicolasH2/ggbrace")}
```

## Inhalt

-   Vorannahmen der Regression prüfen

## Lernziele

## Teaser lineare Regression

### Ein Beispiel

Als Förster\*in in den USA stehst du vor einer Amerikanischen Traubenkirsche (*Prunus serotina*) ([wiki](https://de.wikipedia.org/wiki/Sp%C3%A4tbl%C3%BChende_Traubenkirsche)). Sie soll bald gefällt werden, denn sie hat ein begehrtes Holz, und du brauchst Geld. Am liebsten möchtest wissen, bevor du den Baum fällst, wie viele Kubikmeter Holz wohl dabei herauskommen werden, die du verkaufen kannst. Aber du möchtest keine allzu komplexen Messungen vornehmen. Den besten und einfachsten Schätzer, den wir bequem vornehmen können ist der Durchmesser des Baumes, der sich ungefähr aus dem Umfang errechnen lässt. Dafür braucht es lediglich ein Maßband und $\pi$.

Der Baum, vor dem wir stehen, hat einen Durchmesser von 45 cm. Wie viele Kubikmeter Holz können wir erwarten?

![](images/tree_patrick.jpg){width=60%}

Bild: Kein Kirschbaum, aber zeigt das Prinzip.

*US Army Corps of Engineers. Patrick Bloodgood, photographer. [CC BY 2.0](https://creativecommons.org/licenses/by/2.0), via Wikimedia Commons*

::: infobox
Achtung! **Vereinfachungsalarm**

Wir lassen hier einige Dinge der Einfachheit halber komplett außer Acht, zum Beispiel:

-   in der Forstwirtschaft wird zur Schätzung des Volumens eine etwas kompliziertere Formel verwendet, in die neben dem Durchmesser auch die Höhe mit einfließt
-   Dabei gibt es verschiedene Volumenmaße für Holz mit und ohne Rinde
-   ...
:::

### Beispieldaten

Wir haben bereits Daten von 31 gefällten Bäumen der gleichen Art, und können damit eine Vorhersage treffen. Die Daten befinden sich im `trees`-Datensatz, der in Base R eingebaut ist, und sind der Grund für dieses Beispiel.

Schauen wir uns die Daten mal an:

::: aufgabe
**1.** Wie sehen die ersten 6 Zeilen des Datensatzes `trees` aus?
:::

```{r head, exercise = TRUE}

```

```{r head-solution}
head(trees)
```

#### Ein bisschen Data-Cleaning

::: aufgabe
**2.**

Die Höhe `Height` lassen wir heute außen vor.

Der Durchmesser heißt im Datensatz fälschlicherweise `Girth`, was Umfang bedeutet. (Das ist einfach ein Fehler in der Benennung, siehe `?trees`).

Beheben Sie das, in dem Sie in `trees` eine neue Variable namens `Diameter` erstellen, die den Inhalt der Variable `Girth` enthält.
:::

```{r girth, exercise = TRUE}

```

```{r girth-solution}
trees$Diameter <- trees$Girth
```

::: aufgabe
**3.**

Die Variablen sind (typisch USA) alle in nicht-metrischen Einheiten angegeben.

Rechnen Sie `Diameter` und `Girth` in metrische Einheiten um, damit das Beispiel intuitiver verständlich ist.

`Diameter` = Inch. Ziel: cm

`Volume` = ft³. Ziel: m³

**Hilfestellung zur Umrechung:**

$cm = Inch * 2.54$

$m^3 = ft^3 * 0.0283168466$
:::

```{r feet-setup}
trees$Diameter <- trees$Girth
```

```{r feet, exercise = TRUE}

```

```{r feet-solution}
trees$Diameter <- trees$Diameter * 2.54 # Inch in cm
trees$Volume <- trees$Volume * 0.0283168466 #  ft³ in m³

```

```{r silentsetup}
# Hier drauf sollten sich alle Exercise Code Chunks, die das trees-Dataset verwenden, beziehen

trees$Diameter <- datasets::trees$Girth * 2.54 # Inch in cm
trees$Volume <- datasets::trees$Volume * 0.0283168466 #  ft³ in m³

trees$Diameter_centered <- trees$Diameter - mean(trees$Diameter)
```

```{r silentsetup-global}
# Und das gleiche noch mal fürs Global Environment

trees$Diameter <- datasets::trees$Girth * 2.54 # Inch in cm
trees$Volume <- datasets::trees$Volume * 0.0283168466 #  ft³ in m³

trees$Diameter_centered <- trees$Diameter - mean(trees$Diameter)
```

#### Visualisierung

Ein schneller Scatterplot gibt uns jetzt Auskunft über die Beziehung zwischen Durchmesser und Volumen:

```{r, echo = TRUE}
ggplot(trees, aes(x = Diameter, y = Volume)) +
  geom_point() +
  theme_minimal() +
  labs(x = "Durchmesser (cm)", y = "Volumen (m³)")
```

Grundsätzlich sieht es so aus, dass wir mehr Holz ernten, je dicker der Baum war.

Durchmesser und Volumen hängen also irgendwie proportional zusammen. Für unsere Vorhersage wünschen wir uns aber zu wissen, wie genau diese Proportion aussieht.

Glücklicherweise sieht der Zusammenhang linear aus, das heißt, wir könnten ihn sinnvoll mit einer Gerade beschreiben:

```{r echo=F, message=FALSE, warning=FALSE}
ggplot(trees, aes(x = Diameter, y = Volume)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_minimal() +
  labs(x = "Durchmesser (cm)", y = "Volumen (m³)")
```

Die blaue Gerade ist unser Modell, die Punkte sind die Realität, aus der wir das Modell abgeleitet haben. Ein Modell wird den Komplexitäten der Realität nie komplett gerecht, aber manche Modelle sind trotzdem sehr nützlich. Die Gerade hier zum Beispiel ist ein eher einfaches Modell und beschreibt die Realität trotzdem relativ gut.

Geraden werden beschrieben durch lineare Funktionen. Unser Modell besteht also essentiell aus einer linearen Funktion! Wie genau das alles funktioniert, erfahrt ihr im nächsten Abschnitt. Vorher aber noch ein kleiner Test, ob ihr auch schon mit dem Modell umgehen könnt und die Eingangsfrage lösen könntet:

```{r first}
question_numeric(
  "Wie viel Kubikmeter Holz sind zu erwarten laut der blauen Gerade (unserem linearen Modell) bei einem Durchmesser von 45 cm?",
  answer(1.5, correct = T)
)
```

Super, das waren die ersten Schritte! Im nächsten Kapitel werden die mathematischen Grundlagen aufgefrischt.

![Yay!](./images/shuffling_tree.gif)

## lineare Funktionen

![](./images/function.gif)

Bisher sieht unser Modell so aus:

Wir wissen nicht genau, was drin passiert, nur das wir den **Prädiktor** $x$ (Durchmesser) reingeben und daraus das **Kriterium** $y$ (Volumen) herausbekommen wollen. In diesem Kapitel erklären wir, was im Modell vorgeht - nämlich eine lineare Funktion.

### ein bisschen Begriffsarbeit

Die lineare Funktion kennen sicher alle noch aus der Schule, ca. 8. Klasse:

$$
y = mx + n
$$

oder in anderer Notation:

$$
y = a + bx
$$

Egal welche der Notationen Sie hatten, oder auch wenn Sie noch gar keine Berührungspunkte mit linearen Funktionen hatten, der Aufbau ist immer gleich.

Wir werden diese Notation verwenden:

$$
\hat y = b_0 + b_1\cdot x
$$

-   Dabei steht $\hat y$ für den durch das Modell vorhergesagten Wert, auch engl. "*response variable"* genannt,"*output variable"* oder **Kriterium**. Im Beispiel wäre es das Holzvolumen in m³.

    Ein Dach über der Variable kennzeichnet immer, dass es sich hier um einen geschätzten Wert handelt.

-   $x$ ist der **Prädiktor**, oder unsere *input variable*, im Beispiel der Baumdurchmesser in Inches.

Daraus ergibt sich für uns folgendes Modell:

$$
\hat{Volumen} = b_0 + b_1 \cdot Durchmesser 
$$

#### Was sind $b_0$ und $b_1$?

Beides sind Regressionskoeffizienten - zwei Parameter, die die Lage der Gerade beschreiben.

-   $b_0$ ist die **Regressionskonstante**.

    Das ist der vorhergesagte Wert, wenn $x$ 0 ist. Das ergibt sich aus der Gleichung, wenn man für $x$ 0 einsetzt:

    $\hat y = b_0 + b_1 \cdot 0 = b_0$

    Bei einer Gerade ist $b_0$ also dort, wo $x = 0$. Das ist oft auch dort, wo die y-Achse verläuft, deswegen nennt man $b_0$ auch Achsenabschnitt oder *Intercept* auf Englisch.

-   $b_1$ ist das **Regressionsgewicht**.

    Es entspricht der Steigung der Gerade (engl. *Slope*) und ist inhaltlich die Änderung im vorhergesagten Wert ($\hat y$), wenn man $x$ um eine Einheit erhöht.

    Es erinnern sich bestimmt alle an das Dreieck, was man an die Gerade zeichnen kann, um die Steigung zu bestimmen.

### Beispiel-Gerade

Hier einmal eine Beispiel-Gerade, beschrieben durch die Gleichung $\hat y = 9 - 1.5x$.

-   Intercept $b_0 = 9$
-   Steigung $b_1 = -1.5$

```{r}
.gerade <- function(x, intercept = 0, slope = 1){
  intercept + slope * x
}

ggplot() +
  stat_function(fun = `.gerade`, xlim = c(-1, 10), args = list(slope = -1.5, intercept = 9)) +
  geom_segment(aes(x = 2, y = 6, yend = 6, xend = 3), color = "red", 
               arrow = arrow(length = unit(0.03, "npc"))) +
  geom_segment(aes(x = 3, xend = 3, y = 6, yend = 4.5), color = "red",
               arrow = arrow(length = unit(0.03, "npc"))) +
  annotate("text", x = 2.5, y = 6.5, label = "1") +
  annotate("text", x = 3.5, y = 5.125, label = "-1.5") +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 10), expand = F) +
  scale_y_continuous(breaks = 0:10) +
  scale_x_continuous(breaks = 0:10) +
  theme_minimal()
```

### Quiz

```{r}

ggplot() +
  stat_function(fun = `.gerade`, xlim = c(0, 20),
                args = list(intercept = 5, slope = 0.5)) +
  stat_function(fun = `.gerade`, xlim = c(0, 20),
                args = list(intercept = 10, slope = 1), linetype = "dashed") +
  coord_cartesian(ylim = c(0, 20), xlim = c(0, 20)) +
  theme_minimal()
```

```{r guess}
quiz(
  question_numeric(
  "Welche Steigung (b1) hat die gestrichelte Linie?",
  answer(1, correct = T)
  ),
  question_numeric(
  "Welche Steigung (b1) hat die durchgezogene Linie?",
  answer(0.5, correct = T)
  ),
  question_numeric(
    "Welches Intercept (b0) hat die gestrichelte Linie?",
    answer(10, correct = T)
  ),
  question_numeric(
    "Welches Intercept (b0) hat die durchgezogene Linie?",
    answer(5, correct = T)
  ),
  caption = "Schulmathe auffrischen"
)
```

Alles klar! Wir sind bereit, unser eigenes lineares Modell zu erstellen.

## Fitting the model

Nun wissen wir also, dass das Modell einer einfachen linearen Regression einfach eine Geradengleichung ist, beschrieben durch die Parameter $b_0$ (Intercept) und $b_1$ (Steigung.)

Aber wie genau kommen wir auf diese Parameter? Den Prozess, ein Modell zu erstellen, was die Realität möglichst gut erklärt, nennt man auf Englisch *to fit a model*.

$b_0$ und $b_1$ werden so gewählt, dass die Regressionsgerade einen möglichst kleinen Abstand zu allen Datenpunkten hat.

Probieren Sie selbst aus, Regressionsgerade optimal durch die Datenpunkte zu legen!

```{r shiny_ui, echo=FALSE}
sliderInput("b0", "Intercept b0:", min = -1.5, max = 1.5, value = 0, step = 0.01)
sliderInput("b1", "Slope b1:", min = 0, max = 0.15, value = 0.075, step = 0.001)
plotOutput("distPlot", inline = TRUE)
```

```{r shiny_server, context="server", cache=TRUE}
output$distPlot <- renderPlot({
  
# Prerequisites: Daten müssen noch mal erstellt werden weil shiny app verwendet eigenes environment
trees$Diameter <- datasets::trees$Girth * 2.54 # Inch in cm
trees$Volume <- datasets::trees$Volume * 0.0283168466 #  ft³ in m³
  
# Plot
ggplot(trees, aes(x = Diameter, y = Volume)) +
  geom_point() +
  geom_abline(intercept = input$b0, slope = input$b1, color = "blue") +
  theme_minimal() +
  labs(x = "Durchmesser (cm)", y = "Volumen (m³)") +
  coord_cartesian(xlim = c(0, 60), ylim =c(-2, 2.5), expand = F)

},
width = 800, height = 400)
```

Super! Merken Sie sich Ihre Schätzungen, und vergleichen Sie im Kapitel "Umsetzung in R" mit R's Schätzungen. 

Aber wann liegt die Gerade optimal über den Daten? Es gibt verschiedene Ansätze dazu. 
Ein wichtiges Konzept sind dabei die Residuen. 

### Residuen

```{r residplot, echo=F, message=FALSE, warning=FALSE}
lm(Volume ~ Diameter, data = trees) -> fit
fitted(fit) -> fitted

ggplot(trees, aes(x = Diameter, y = Volume)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  geom_segment(aes(xend = Diameter, yend = fitted, color = "resid")) +
  scale_color_manual(values = c(resid = "darkred"), labels = c(resid = "Residuen")) +
  theme_minimal() +
  labs(x = "Durchmesser (cm)", y = "Volumen (m³)", color = "")
```

Die oben in dunkelrot gekennzeichneten vertikalen Abstände zwischen realen Werten und den vorhergesagten Werten nennt man auch **Residuen**, also "Überbleibsel". Residuen stellen die Abweichung der Realität vom Modell dar, also den "Fehler", den das Modell nicht erklären kann.

Offensichtlich ist es daher besser, wenn die Residuen kleiner sind, als wenn sie sehr groß sind. Kleine Residuen bedeuten, dass das Modell näher an der Realität liegt.

#### Formel

Ein Residuum der Messung $i$ errechnet sich aus: $y_i - \hat y_i$.

#### Methode der kleinsten Quadrate

Eine Möglichkeit, zu definieren wann eine Regressionsgerade optimal an die Daten angepasst ist, zu sagen:

> „Die Regressionsgerade liegt optimal, wenn die Summe der quadrierten Residuen minimal ist.“

Diese Definition verwendet zur Schätzung der Regressionskoeffizienten die Methode der kleinsten Quadrate (*least squares*), was auch aus gutem Grund die populärste ist - sie minimiert die Residuen und erreicht damit die größtmögliche Varianzaufklärung durch das Modell. 

Natürlich hat *least squares estimation* auch Nachteile, zum Beispiel durch das Quadrieren hohe Empfindlichkeit gegenüber Extremwerten. Deswegen sei hier gesagt: es gibt auch andere Methoden, jedoch ist *least squares* der Standard, und das ist auch gut so. 

<details>
<summary><a>▼ * für's Interesse: mathematische Formeln</a></summary>
::: infobox
**Woher kommen die Formeln für** $b_1$ und $b_0$?

Wie bereits gesagt, wird die Gerade so gelegt, dass die Summe der quadrierten Residuen minimal ist.

$$
\sum_{i = 1}^n \left( y_i - \hat y_i \right)^2 = min
$$ mit:

-   $y_i$ = tatsächlicher Wert

-   $\hat y_i$ = vorhergesagter Wert

Analog zur Berechnung der Varianz wird hier quadriert. Da quadrierte Werte immer positiv sind, erreicht man, dass sich negative und positive Abweichungen nicht aufheben beim Summieren. Außerdem stellt das Quadrieren weitere günstige mathematische Eigenschaften für das Minimieren her.

Einsetzen der Regressionsgleichung für $\hat y$:

$$
\sum_{i = 1}^n \left( y_i - (b_0 + b_1 \cdot x_i) \right)^2 = min
$$

Minimieren: Die Herleitung allgemeiner Formeln für minimale Residuen funktioniert über das Nullsetzen der partiellen Ableitung nach $b_0$ und der partiellen Ableitung nach $b_1$. Es gibt gute externe Quellen dazu, z. B. im [wikibook über lineare Regression](https://de.wikibooks.org/wiki/Statistik:_Regressionsanalyse#Einfaches_lineares_Regressionsmodell).

hier sind die fertigen Formeln für $b_0$ und $b_1$:

```{=tex}
\begin{align}
b_1 &= \frac{cov(x,y)}{var(x)}
\\
b_0 &= \bar y - b_1 \cdot \bar x
\end{align}
```
Der Key-Takeaway ist: Diese Formeln liefern per Definition die optimale Regressionsgerade mit minimalen Residuen!

Amazing.
:::
</details>
</br>
<details>
<summary><a>▼ \* Exkurs: lineare Regression und Machine Learning</a></summary>
::: infobox
Man kann die Quadrierten Residuen auch minimieren durch Trial & Error, wie Sie es oben selbst vermutlich auch gemacht haben. Das geht super mit Machine-Learning-Algorithmen. Wir werden keine dieser Algorithmen in R verwenden, aber ich möchte darauf hinweisen, dass es auch mit Machine Learning geht, und die lineare Regression zu den am häufigsten angewandten Machine-Learning-Algorithmen zählt. 

Hier eine Illustration des Fitting-Prozesses, allerdings mit anderen Daten. Die Grafik zeigt wie ein Machine-Learning-Algorithmus sich Schritt für Schritt der optimalen Regressionsgerade annähert, und die mittleren quadrierten Residuen (Mean Squared Error, MSE) immer kleiner werden, bis es nicht kleiner geht.

![](images/iterative_fitting.gif){width="80%"}

Bild: [ghbat.com](https://gbhat.com/machine_learning/linear_regression.html)

:::
</details>
</br>
Hervorragend, das theoretische Grundlagenwissen ist nun vorhanden, so dass Sie sich einem R-Output stellen können!

## Umsetzung in R

In R erhält man die Regressionskoeffizienten $b_0$ und $b_1$ über die Funktion `lm()` (*linear model*).

Fokussieren wir uns zunächst auf die Eingabe, den Output schauen wir uns im nächsten Schritt an.

#### Eingabe

``` r
# Modell erstellen und abspeichern 
fit <- lm(Volume ~ Diameter, data = trees)

# Modell ansehen
summary(fit)
```

##### Code Breakdown

`lm()`:

-   `Volume ~ Diameter`: ist eine Formel, die R sagt: *explain Volume by Diameter*.

    Ich finde es hilfreich, die Tilde im Kopf zu lesen als: *explained by* oder auf deutsch "erklärt durch". Links der Tilde kommt immer die Variable hin, die wir erklären oder vorhersagen wollen, und rechts der Tilde die Prädiktoren.

-   `data = trees`: Da wir die die Variablennamen verwenden ohne `trees$...` davor, sagen wir R noch im Argument `data`, wo die Variablen zu finden sind.

`fit <-`:

-   Schließlich speichern wir das Modell in einem Objekt, was wir `fit` genannt haben, was ein frei ausgewählter Name ist.

`summary(fit)`:

-   ruft eine Zusammenfassung unseres gespeicherten Modells auf, was uns zur Ausgabe führt.

#### Ausgabe

Der Output sieht eventuell überwältigend aus, weil er eine ziemlich hohe Informationsdichte hat. Das macht aber gar nichts, denn wir richten unsere Aufmerksamkeit gezielt auf den Abschnitt "Coefficients:". Dort finden wir die Parameter $b_0$ (Intercept) und $b_1$ (Steigung). Und zwar in der Spalte "Estimate".

```         
Call:
lm(formula = Volume ~ Diameter, data = trees)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.228386 -0.087972  0.004303  0.098961  0.271468 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.046122   0.095290  -10.98 7.62e-12 ***
Diameter     0.056476   0.002758   20.48  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1204 on 29 degrees of freedom
Multiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 
F-statistic: 419.4 on 1 and 29 DF,  p-value: < 2.2e-16
```

Daraus können wir entnehmen:

-   Intercept $b_0 = -1.0461$
-   Steigung $b_1 = 0.0565$

Alle anderen Spalten interessieren uns im Moment nicht.

```         
Coefficients:
             Estimate    
(Intercept) -1.046122   <--- b0
Diameter     0.056476   <--- b1
---
```

::: aufgabe
Vergleichen Sie Ihre Schätzung aus dem letzten Kapitel vom Experimentieren mit den Slidern mit den realen Werten!
:::

<details>

<summary><a>▼ \* Geek Stuff: Welchen Algorithmus verwendet `lm()`?</a></summary>
::: infobox
Lustigerweise wird unter der Haube von `lm()` letztlich nach vielen Zwischenschritten eine uralte Funktion in einer anderen Programmiersprache, nämlich FORTRAN, aufgerufen, um die Schwerarbeit zu machen. Sie stammt aus dem `LINPACK`-Paket, was original für Supercomputer in den 70ern und 80ern verfasst wurde, um lineare Gleichungssysteme zu lösen. 

Hier gibt es einen interessanten Blogbeitrag [(A deep dive into how R fits a linear model)](https://madrury.github.io/jekyll/update/statistics/2016/07/20/lm-in-R.html) dazu, der jeden einzelnen Schritt auflistet bis zur untersten Ebene, dem FORTRAN-Herzstück von `lm()`. 

Der verwendte Algorithmus basiert auf QR-Zerlegung, und verwendet eine Householder-Transformations, um zu den Koeffizienten mit den kleinstmöglichen quadrierten Residuen zu gelangen. Ich weiß nicht, was das ist - aber [Wikipedia](https://de.wikipedia.org/wiki/QR-Zerlegung) weiß es :)   
:::
</details>
</br>

Jetzt sind Sie dran!

::: aufgabe
**1.**

Erstellen Sie mit `lm()` ein lineares Modell zur Erklärung des Volumens durch den Durchmesser im `trees`-Datensatz und speichern Sie es ab als `fit`.
:::

```{r model, exercise = TRUE}

```

```{r model-solution}
fit <- lm(Volume ~ Diameter, data = trees)
```

::: aufgabe
**2.**

Lassen Sie sich mit `summary()` eine Zusammenfassung des gespeicherten Modells ausgeben!

Finden Sie $b_0$ und $b_1$!
:::

```{r summary-setup}
trees$Diameter <- trees$Girth * 2.54 # Inch in cm
trees$Volume <- trees$Volume * 0.0283168466 #  ft³ in m³
fit <- lm(Volume ~ Diameter, data = trees)
```

```{r summary, exercise = TRUE}

```

```{r summary-solution}
summary(fit)
```

## Interpretation der Koeffizienten

Jetzt haben wir $b_0 = -1.0461$ und $b_1 = 0.0565$ herausgefunden - aber was heißen sie eigentlich inhaltlich?

Zunächst einmal können wir uns merken, dass die Regressionskoeffizienten *immer in Einheiten der vorhergesagten Variable*, in unserem Fall also Volumen (m³) sind.

### $b_0$ (Intercept)

> „$b_0$ ist der vorhergesagte Wert, wenn der Prädiktor den Wert 0 annimt."

Bezogen auf das Beispiel:

„Wenn der Durchmesser eines Baumes 0 cm betragen würde, wäre das vorhergesagte Holzvolumen -1.05 Kubikmeter." --- Das macht inhaltlich wenig Sinn - es gibt keinen Baum, wenn er einen Durchmesser von 0 cm hat, und es gibt auch kein negatives Volumen.

Das ist mit allen Daten so, wo 0 nicht im sinnvollen Wertebereich ist. Man kann dann für eine bessere Interpretierbarkeit die Daten zentrieren, so dass 0 ein sinnvoller Wert ist. Siehe Abschnitt Transformation.

### $b_1$ (Steigung)

> „$b_1$ ist die vorhergesagte Änderung, wenn der Prädiktor um eine Einheit erhöht wird."

„Pro Zentimeter Durchmesser mehr steigt also unser erwartetes Volumen um 0.0565 Kubikmeter."

## Transformationen

Für die lineare Regression müssen das Kriterium $y$ und der Prädiktor $x$ beide metrisch sein, also mindestens intervallskaliert. Ab dem Intervallskalenniveau sind Transformationen zulässig, solange sie die Proportion erhalten.

| Transformation | Daten        |
|----------------|--------------|
| $x$ (Original) | 10, 20, 30   |
| $x \cdot 5$    | 50, 100, 150 |
| $x - 20$       | -10, 0, 10   |

: Beispiel für proportionale Transformation

Eine dieser proportionserhaltenden Transformationen ist das Zentrieren.

### Zentrieren

Beim Zentrieren wird von jedem Wert der Mittelwert abgezogen. Das entspricht der letzten Tabellenzeile, weil 20 der Mittelwert der Zahlenreihe ist. Die transformierten Daten bilden dann die Differenz zum Mittelwert ab.

#### Formel

$$
x' = x - \bar x
$$ Sie sind dran!

::: aufgabe
**1.** Zentrieren Sie die Variable `Diameter` aus dem `trees`-Datensatz und speichern sie das Ergebnis als eine neue Spalte namens `Diameter_centered` ab.
:::

```{r center, exercise = TRUE, exercise.setup = "silentsetup", exercise.caption = "Zentrieren"}

```

```{r center-solution}
trees$Diameter_centered <- trees$Diameter - mean(trees$Diameter)
```

```{r meancenter}
question_numeric("2. Was ist der Mittelwert einer zentrierten Variable und warum ist das so? Prüfen Sie notfalls im obigen Codeblock nach.",
                 answer(0, correct = TRUE, message = "Richtig! Der Mittelwert einer zentrierten Variablen ist immer 0."))
```

#### \* fürs Interesse

<details>

<summary><a>▼ Klicken Sie `HIER` für einen mathematischen Beweis</a></summary>

::: infobox
Es ist auch mathematisch zu zeigen, dass der Mittelwert einer zentrierten Variable immer 0 ist:

```{=tex}
\begin{align}
\mbox{Allgemeine Formel für arithmetisches Mittel} & ~ & \bar x' = \frac{\sum_{i = 1}^n x_i'}{n} &= 0 \\
\mbox{Einsetzen der Zentrierungsformel} & ~ & \frac{\sum_{i = 1}^n (x_i - \bar x)}{n} &= 0 \\
\mbox{Aufspalten der Summe in mehrere Summen} & ~ & \frac{\sum_{i = 1}^n x_i - \sum_{i = 1}^n \bar x}{n} &= 0 \\
\mbox{\(\bar x\) ist eine Konstante - Summe einer Konstanten = \(n \cdot Konstante\)} & ~ & \frac{\sum_{i = 1}^n x_i - n \cdot \bar x}{n} &= 0 \\
\mbox{Einsetzen der Formel für \(\bar x\)} & ~ & \frac{\sum_{i = 1}^n x_i - n \cdot \frac{\sum_{i = 1}^n x_i}{n}}{n} &= 0 \\
\mbox{Kürzen} & ~ & \frac{\sum_{i = 1}^n x_i - \sum_{i = 1}^n x_i}{n} &= 0 \\
\mbox{was zu zeigen war: 0 ist 0} & ~ & \frac{0}{n} &= 0
\end{align}
```
:::

</details>

#### Modell mit zentriertem Prädiktor

Jetzt können wir das Modell noch einmal mit der zentrierten Variable rechnen:

```{r centernew, exercise = TRUE, exercise.setup = "silentsetup"}
fit_centered <- lm(Volume ~ Diameter_centered, data = trees)
summary(fit_centered)
```

```{r centerquestion}
quiz(
question_numeric("Wie lautet $b_0$ (*Intercept*) (auf drei Nachkommastellen)?",
                 answer(0.854347, correct = T),
                 answer(0.854, correct = T)),
question_numeric("Wie lautet $b_1$ (*Slope*) (auf drei Nachkommestellen)?",
                 answer(0.056476, correct = T),
                 answer(0.056, correct = T)),
caption = "3. Koeffizienten finden")
```

Am Modell hat sich nichts geändert außer dem *Intercept*.

Durch das Zentrieren haben wir 0 zu einem sinnvollen Wert gemacht, der auch tatsächlich durch die Daten abgedeckt ist.

Stellen wir das visuell dar:

Nur die x-Achse ist anders skaliert, die Proportionen sind aber erhalten.

```{r echo=FALSE, message=FALSE, warning=FALSE}
uncentered <- ggplot(trees, aes(x = Diameter, y = Volume)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_minimal() +
  labs(x = "Durchmesser (cm)", y = "Volumen (m³)")

centered <- ggplot(trees, aes(x = Diameter_centered, y = Volume)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_minimal() +
  labs(x = "Durchmesser (cm), zentriert", y = "Volumen (m³)")

gridExtra::grid.arrange(uncentered, centered)
```

Bei den unzentrierten Daten ist 0 nicht im Wertebereich enthalten und lässt sich nicht sinnvoll interpretieren.

Nach dem Zentrieren ist 0 ein sinnvoller Wert, nämlich der Mittelwert!

Die Steigung $b_1$ ändert sich durch das Zentrieren nicht.

#### Interpretation zentrierte Prädiktoren

```{r questioncenter}
question_checkbox("4. Wie würden Sie $b_0$ jetzt interpretieren, wo der Prädiktor zentriert wurde?",
                  answer("Bei einem mittleren Baumdurchmesser sagt das Modell 0.854 m³ Holzernte voraus.", correct = TRUE, message = "$b_0$ ist der vorhergesagte Wert, wenn der Prädiktor den Wert 0 annimt. Da nun 0 dem Mittelwert entspricht, können wir von einem mittleren Durchmesser sprechen"),
                  answer("Wenn man das Volumen um eine Einheit erhöht, steigt der mittlere Durchmesser um 0.854 cm", correct = FALSE, message = "Hier ist alles verdreht. Zunächst, das Volumen ist unser Kriterium, also können wir daraus nicht den Durchmesser vorhersagen. Aber selbst dann wäre das Schema „Wenn man Prädiktor um 1 Einheit erhöht, um wie viel ändert sich dann die Vorhersage?“ für die Interpretation für das Regressionsgewicht $b_1$ geeignet und nicht für die Regressionskonstante $b_0$. Zuletzt: Die Koeffizienten liegen immer in Einheiten des Kriteriums vor, also ist cm hier auch nicht die richtige Einheit."),
                  answer("Bei einem hypothetischen Durchmesser von 0.854 cm nimmt das vorhergesagte Volumen den Mittelwert an", correct = F, message = "Das stimmt leider nicht, denn die Koeffizienten liegen immer in Einheiten des Kriteriums vor, also in diesem Fall m³. Und dann ist auch noch der Mittelwert an der falschen Stelle, nämlich eigentlich ist 0.854 das vorhergesagte Volumen in m³ bei einem Durchmesser von 0 - da wir aber zentriert haben, entspricht das einem mittleren Durchmesser.")
                  )
```

```{r importantcenter}
question_text("5. Haben Sie das Wichtigste mitgenommen? Was repräsentiert der Wert 0 bei einer zentrierten Variablen? (Stichwort)", 
            correct = "Super! Sie haben das Wichtigste verstanden.", incorrect = "Probieren Sie es nochmal (vielleicht eine andere Schreibweise). Zentrierte Daten haben als Wert ihren Abstand zum Mittelwert. Der Mittelwert hat den Abstand 0 zum Mittelwert",
                 answer("den Mittelwert", correct = T),
                 answer("arithmetisches Mittel", correct = T),
                 answer("Mittelwert", correct = T)
)
```

Sie sind schon sehr weit gekommen! Schauen wir uns jetzt mal an, was noch so im R-Output zu finden ist.

![](images/tree_dance.gif)

## Modellgüte $R^2$

Konzentrieren wir uns nun auf einen neuen Bereich im R-Output:

```{r modelgoodness, exercise = T, exercise.eval = TRUE}
fit_centered <- lm(Volume ~ Diameter_centered, data = trees)
summary(fit_centered)
```

Das Bestimmtheitsmaß (auch Determinationskoeffizient) $R^2$ finden wir in der vorletzten Zeile, bei:

```         
Multiple R-squared:  0.9353   <------- R²
```
<details>
<summary><a>▼ Warum heißt es im Output „Multiple R-squared“?</a></summary>
::: infobox
Der Präfix "Multiple" ist wie eine Warnung: Wenn mehr als ein Prädiktor im Modell verwendet werden, wird $R^2$ mit jedem Prädiktor zwangsläufig höher, außer man korrigiert diese Verzerrung. Das heißt dann `Adjusted R-squared` und das ignorieren wir hier alles, weil wir nur einfache lineare Regression machen, also nur einen Prädiktor verwenden.
:::
</details>

#### Was ist $R^2$?

$R^2$ definiert, wie gut ein Modell an die Daten angepasst ist und heißt deswegen auch „Anpassungsgüte“. 

Um $R^2$ zu verstehen, ist es hilfreich zu verstehen, dass man die Varianz im Kriterium (Schwankungen im Holzvolumen) aufteilen kann in zwei Teile: 

- in einen durch das Modell (0.056 * Durchmesser - 1.04) erklärten Teil 
- und in den Fehler, der nicht erklärbare zufällige Abweichung vom Modell ist.

Aus beiden Teilen zusammen ergibt sich die Gesamtvarianz des Kriteriums. 

Die Formel für $R^2$ ist dann: 

$$
R^2 = \frac{\text{durch das Modell erklärte Varianz in y}}{\text{Gesamte Varianz in y}} = \frac{\sum_{i=1}^n( \hat y_i - \bar y)^2}{\sum_{i=1}^n(y_i - \bar y)^2}
$$

### Interpretation 

$R^2 = 0.9353$

> „93,53% der Varianz im Holzvolumen können durch das Modell aufgeklärt werden.“ 

Das Modell besteht in diesem Fall nur aus einem Prädiktor, deswegen können wir auch „Modell“ durch den Prädiktor ersetzen:

„93,53 % der Varianz im Holzvolumen können auf den Prädiktor "Durchmesser" zurückgeführt werden.“

Die Höhe des Wertes ist nicht wirklich gut vergleichbar, da es vom Forschungsgebiet abhängt, wie viel Varianzaufklärung als "gut" gilt. Über 90% Varianzaufklärung durch das Modell sind aber fast schon unglaublich und das kommt in der Praxis eher selten vor.  

### Zusammenhang mit Korrelationskoeffizient $r$

Im Fall der einfachen linearen Regression, aber nicht bei multipler Regression, entspricht $R^2$ der quadrierten Pearson-Korrelation $r$ des Prädiktors mit dem Kriterium. Daraus folgt: $\sqrt{R^2} = r$

#### Übung
::: aufgabe
Berechnen Sie den Korrelationskoeffizienten auf zwei Wegen: 

- als Wurzel von $R^2$
- mit der `cor()`-Funktion

Sind die Ergebnisse identisch?
:::

```{r cor, exercise = TRUE, exercise.setup = "silentsetup", exercise.cap = "Korrelation"}
r2 <- 0.9353

```

```{r cor-solution}
r2 <- 0.9353
sqrt(r2)
cor(trees$Diameter, trees$Volume) 
# 0.967109 == 0.967112
# Die kleine Abweichung ist ok, wir runden auf 4 Nachkommastellen, weil R² uns nicht präziser gegeben ist.
```

```{r korrelationsfrage}
question_numeric("Geben Sie Ihren errechneten Wert der Pearson-Korrelation zwischen Volumen und Durchmesser ein, gerundet auf 4 Nachkommastellen!",
                 answer(0.9671, correct = TRUE, message = "Wir runden auf 4 Nachkommastellen, weil R² uns nicht präziser gegeben ist."))
```

#### Reminder: Interpretation Korrelation
Wie interpretiert man noch mal den Korrelationskoeffizienten? Siehe Tutorial Korrelation.

- Stärke des Zusammenhangs (alles ab ca. |.5| ist ein starker Zusammenhang)
- Richtung des Zusammenhangs

„0.9671 ist ein starker positiver Zusammenhang“.

Nun haben Sie sich ausgiebig mit der Modellgüte befasst. Allerdings können wir damit noch keine allgemeinen Aussagen treffen über Kausalität, denn $R^2$ ist ein rein deskriptives, also *beschreibendes* Maß. Um Schlussfolgerungen abzuleiten, müssen wir ein bisschen Inferenzstatistik betreiben (schließende Statistik).

## Signifikanztests

### Test des Gesamtmodells

Die Problemstellung ist folgende: 

Alles, was wir bisher beobachtet haben, gilt zwar in unserer kleinen Stichprobe von 31 Amerikanischen Traubenkirschen, aber wir wissen nicht, ob wir die Ergebnisse auch verallgemeinern können. 

Haben wir nicht vielleicht eine Stichprobe gezogen, in welcher der Durchmesser und das Holzvolumen **zufällig** zusammenhängen, obwohl das eigentlich generell im ganzen Wald nicht der Fall ist?

![](images/ftest.jpg){width=80%}

Die Nullhypothese $H_0$ des Tests lautet: $R^2$ beträgt in Wahrheit 0. 

Was der Signifikanztest tut, ist zu schauen: Wie groß ist die Wahrscheinlichkeit $p$, die vorliegende Stichprobe (oder eine noch extremere) aus einer Population zu ziehen, in der die Nullhypothese gilt?

Wie groß ist die Wahrscheinlichkeit, in den Wald zu gehen, 31 Bäume zu fällen, bei denen der Durchmesser zufällig 93% der Varianz im Volumen aufklären kann, obwohl im ganzen Wald der Durchmesser eigentlich nichts mit dem Volumen zu tun hat? 

Wenn es sich als sehr unwahrscheinlich herausstellt, gewinnen wir gewissermaßen rückwärts die Erkenntnis, dass die Nullhypothese aller Wahrscheinlichkeit nach nicht in der Population gilt.

Aber ab wann ist etwas sehr unwahrscheinlich?

Das sind arbiträr festgelegte Grenzen, die sich je nach Fachgebiet unterscheiden, aber ein allgemeiner Standard ist ein Signifikanzniveau $\alpha = 0.05\ \ (5\%)$.

#### R-Output

Wir finden den Test des Gesamtmodells in der allerletzten Zeile des R-Outputs:

```{r ftest_output, exercise = T, exercise.eval = TRUE, exercise.setup = "silentsetup"}
fit_centered <- lm(Volume ~ Diameter_centered, data = trees)
summary(fit_centered)
```

Der Test des Gesamtmodells ist ein $F$-Test, da zwei Varianzen ins Verhältnis gesetzt werden:
Die durch das Modell erklärte Varianz, und die nicht erklärte Varianz. (Immer, wenn Varianzen verglichen werden, klingt das in Ihren Ohren nach $F$-Test. Zumindest sollte das so sein.)

Wie genau die `F-statistic` zustande kommt und was es mit den Freiheitsgraden `DF` auf sich hat, kann hier nicht ausführlich behandelt werden. Das sind nur Zwischenschritte auf dem Weg zur Berechnung des $p$-Werts. Wichtig ist, dass Sie den $p$-Wert als Endresultat interpretieren können, egal ob er von einem $F$-, $t$- oder $z$-Test ist. 

Das hier ist die relevante Zeile:

```
F-statistic: 419.4 on 1 and 29 DF,  p-value: < 2.2e-16   <--- p-Wert Gesamtmodelltest
```

### Interpretation des $p$-Werts

<details>
  <summary><a>▼ Wie wird `2.2e-16` gelesen?</a></summary>
  </br>
  
::: vorteile
  Keine Panik - das ist wissenschaftliche Notation (*scientific notation*).
  
  Das wird verwendet, um sehr viele Nullen kompakt darzustellen. 
  
  So ist das Schema:
  
  `2.2e-16`$= 2.2 \cdot 10^{-16} = 0\overbrace{.0000000000000002}^{\substack{\text{Komma} \\ \text{16 Stellen nach vorne}}}2$
  
  - `e` steht für „mal 10 hoch Exponent“
  
  - `-16` ist der Exponent
  
Ein Positiv-Beispiel wäre:

`5.12e+12`$= 5.12 \cdot 10^{12} = 5\overbrace{120\ 000\ 000\ 000}^{\substack{\text{Komma} \\ \text{12 Stellen nach hinten}}}.0$
  
:::
</details>
</br>

Der $p$-Wert ist eine Antwort auf folgende Frage: „Unter der Annahme, dass $R^2$ in der Population 0 ist — Wie wahrscheinlich ist es, zufällig eine Stichprobe zu ziehen in der das vorliegende oder ein noch extremeres $R^2$ zustande kommt?“

> Angenommen, $\alpha = 0.05$:
> 
> Wenn $p \le 0.05$ -> unwahrscheinlich -> Nullhypothese verwerfen
>
> Wenn $p > 0.05$ -> wahrscheinlich -> Nullhypothese beibehalten

```{r pquestion}
learnr::question_radio("Wie würde Ihre Hypothesenentscheidung für das Beispiel aussehen? (α = 0.05)",
               answer("Nullhypothese beibehalten", message = "Da p mit 0.00…22 kleiner als 0.05 ist, sollten wir die Nullhypothese verwerfen."),
               answer("Nullhypothese verwerfen", correct = TRUE, message = "Da p unglaublich klein ist, auf jeden Fall kleiner als das Signifikanzniveau von 0.05, ist es richtig, die Nullhypothese zu verwerfen."),
               allow_retry = TRUE
)
```

#### häufige inkorrekte Formulierungen

Da wir für die Errechnung des $p$-Werts eine Bedingung annehmen, nämlich dass die Nullhypothese gilt, ist der $p$-Wert eine *bedingte* Wahrscheinlichkeit.

Aussagen wie: <s>„Mit einer Wahrscheinlichkeit von [$p$ %] gilt die Nullhypothese“</s> sind deshalb **falsch**. Ob die Nullhypothese in Wahrheit gilt oder nicht, werden wir nie herausfinden - es sei denn, wir messen die ganze Population - aber wir können schätzen, wie wahrscheinlich es ist, zufällig das vorliegende Ergebnis oder ein extremeres zu ziehen, wenn wir annehmen, dass die Nullhypothese gilt - und daraus wiederum Rückschlüsse ziehen darüber, ob wir die Nullhypothese verwerfen oder beibehalten.

### Grafische Darstellung

Da $p$ in diesem Fall unglaublich klein ist, ist es schwierig, alle relevanten Teile in einem Plot unterzubringen. Deswegen gibt es 3: 

- Übersicht (komplett herausgezoomt)
- Zoom Nr. 1 
- Zoom Nr. 2

```{r uebersichtplot, message=FALSE, warning=FALSE}

### Hilfsfunktion definieren, um Ausschnitte aus der F-Funktion zu generieren

.pshade <- function(x, min, max, df1, df2) {
    y <- df(x = x, df1 = df1, df2 = df2)
    y[x < min  |  x > max] <- NA
    return(y)
}

### Parameter definieren

alpha = 0.05
df1 = 1
df2 = 29
detail = 800  # Detailgrad des Plots
f_krit <- qf(1 - alpha, df1, df2)
f_emp <- 419.4  # aus dem R-output übernommen
xlim <- f_krit + 2

### Übersichts-Plot
ggplot() +
  stat_function(geom = "area",
                fill = "4",
                alpha = .4,
                fun = .pshade, 
                args = list(min = 0, max = f_krit, df1 = df1, df2 = df2),
                n = detail) +
  stat_function(geom = "area",
                fill = "7",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_krit, max = xlim, df1 = df1, df2 = df2),
                n = detail) +
  stat_function(geom = "area",
                fill = "red",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_emp, max = xlim, df1 = df1, df2 = df2),
                n = detail) +  
  geom_function(fun = df, args = list(df1 = df1, df2 = df2), n = detail) +
  annotate("segment", x = f_emp, xend = f_emp, y = 0, yend = 0.1) +
  annotate("label", y = 0.1, x = f_emp, label = "empirischer\nF-Wert\n= 419") +
  annotate("segment", xend = f_krit, x = 50, yend = 0, y = 0.1, 
           arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  annotate("label", x = 50, y = 0.1, label = "kritische\nGrenze") +
  xlim(0, f_emp + 100) +
  labs(x = "F(1, 29)", y = "Wahrscheinlichkeitsdichte", title = "Übersicht", subtitle = "F-Verteilung") +
  theme_minimal()

```


In der Übersicht sehen Sie den empirischen (=beobachteten) F-Wert, der sich aus der Varianzaufklärung $R^2$ des Modells berechnen lässt. Er entspricht `F-statistic` im R-Output. 

Hier noch mal die relevante Zeile aus dem Output:

```
F-statistic: 419.4 on 1 and 29 DF,  p-value: < 2.2e-16
```

<details>
  <summary><a>▼ \* Hintergrundwissen: Wie wird der $F$-Wert berechnet?</a></summary>
  </br>
  
::: infobox
\begin{align*}
F &= \frac{R^2}{1-R^2} \\
F &= \frac{\text{durch das Modell erklärte Varianz}}{\text{nicht erklärbare Varianz}}
\end{align*}
:::
</details>
</br>

Die blaue Fläche repräsentiert 95% der Fläche unter der Kurve. Nach den 95% beginnen die restlichen 5% der Fläche, in gelb, und dazwischen liegt die kritische Grenze. Ist der empirische F-Wert größer als die kritische Grenze, ist er innerhalb der gelben Fläche, also innerhalb den äußersten 5%. Ab diesem Punkt wird die Nullhypothese als "unwahrscheinlich" verworfen - der Test wird signifikant. 

```{r zoom1, message = F, warning = F}
### Zoom 1
ggplot() +
  stat_function(geom = "area",
                fill = "4",
                alpha = .4,
                fun = .pshade, 
                args = list(min = 0, max = f_krit, df1 = df1, df2 = df2),
                n = detail) +
    stat_function(geom = "area",
                fill = "7",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_krit, max = xlim, df1 = df1, df2 = df2),
                n = detail) +
    geom_function(
            fun = df, args = list(df1 = df1, df2 = df2), n = detail) +
    annotate("segment", x = mean(c(f_krit, xlim)), y = 0.1, xend = mean(c(f_krit, xlim)), yend = 0.008,
         arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("segment", x = f_krit, y = 0.1, xend = f_krit, yend = 0) +
    annotate("label", x = f_krit, y = 0.12, label = "kritische\nGrenze") +
    annotate("label", x = mean(c(f_krit, xlim)), y = 0.12, label = "5 % der\nFläche") +
    annotate("label", x = 1, y = 0.1, label = "95 % der\nFläche") +
    scale_x_continuous(limits = c(0, xlim)) +
    coord_cartesian(ylim = c(0, 0.5), xlim = c(0, xlim), expand = F) +
    labs(x = "F(1, 29)", y = "Wahrscheinlichkeitsdichte", title = "Zoom Nr. 1",
         subtitle = "Streckung der x-Achse") +
    theme_minimal()


```


#### Wo ist der $p$-Wert?

Die Fläche unter Kurve ab dem empirischen $F$-Wert bis $+\infty$ entspricht dem $p$-Wert.

Dieser ist in diesem Fall so klein, dass man erst sehr weit hineinzoomen muss (siehe Zoom Nr. 2). Dort kann man den $p$-Wert als Fläche erkennen. (Remember, im Output stand ja bereits, $p < 2.2 \cdot 10^{-16}$, was 15 Nullen nach dem Komma entspricht - in der Übersicht überhaupt nicht sichtbar.)


```{r fplots, message=FALSE, warning=FALSE}
### Zoom 2

xlim_small <- f_emp + 100
ylim_small <- 1e-19
ggplot() +
    stat_function(geom = "area",
                fill = "4",
                alpha = .4,
                fun = .pshade, 
                args = list(min = 0, max = f_krit, df1 = df1, df2 = df2),
                n = detail) +
    stat_function(geom = "area",
                fill = "7",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_krit, max = xlim_small, df1 = df1, df2 = df2),
                n = detail) +
      stat_function(geom = "area",
                fill = "red",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_emp, max = xlim_small, df1 = df1, df2 = df2),
                n = detail) +
    geom_function(
            fun = df, args = list(df1 = df1, df2 = df2), n = detail) +
    annotate("segment", x = mean(c(f_emp, xlim_small)), yend = 0.2e-20, xend = mean(c(f_emp, xlim_small)), y = 1e-20, # p-Pfeil
         arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("segment", x = 50, yend = 0, xend = f_krit, y = 2.5e-20, # Pfeil krit. Gr.
         arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("label", x = 50, y = 2.5e-20, label = "kritische\nGrenze") +           # Label kritische Grenze
    annotate("segment", x = 50, yend = 7.5e-20, xend = 2, y = 7.5e-20, # Pfeil krit. Gr. # Pfeil 95%
         arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("label", x = 50, y = 7.5e-20, label = "95 % der\nFläche") + # Label 95%
    annotate("label", x = mean(c(f_emp, xlim_small)), y = 1.2e-20, label = "Fläche\n= p-Wert") + # p-Label
    annotate("label", x = 200, y = 5e-20, label = "5 % der\nFläche") +
    annotate("segment", x = f_emp, y = 0, yend = 5e-20, xend = f_emp) +
    annotate("label", y = 5e-20, x = f_emp, label = "empirischer\nF-Wert\n= 419") +
    xlim(0, xlim_small) +
    coord_cartesian(ylim = c(0, ylim_small), xlim = c(0, xlim_small), expand = F) +
    labs(x = "F(1, 29)", y = "Wahrscheinlichkeitsdichte", title = "Zoom Nr. 2",
         subtitle = "Streckung der y-Achse") +
    theme_minimal()

```

#### zum Vergleich: nicht signifikant

Hier noch mal der entgegengesetzte Fall:

Falls der empirische $F$-Wert innerhalb der blauen 95% liegen würde, die als „wahrscheinlich“ gelten - wie würde unsere Hypothenentscheidung dann lauten?

Wir würden die Nullhypothese beibehalten, da $p > \alpha = 5.21 \%> 5\%$. 

Die Wahrscheinlichkeit, die beobachtete Stichprobe aus einer Population zu ziehen, in welcher die Nullhypothese gilt, ist höher als die von uns angesetzte Grenze (Signifikanzniveau), ab der etwas als „unwahrscheinlich“ gilt.

```{r notsignificantplot, message=FALSE, warning=FALSE}
# devtools::install_github("NicolasH2/ggbrace")
# WARNING! GITHUB DEPENDENCY, wird wahrscheinlich nicht automatisch gehandled - momentan habe ich das testweise verlegt auf setup chunk
# library(ggbrace)

### Parameter festlegen
f_emp <- 3.0
xlim_new <- f_krit + 1
p <- df(3, df1, df2) # 0.052
### PLOT 
### Zoom 1
ggplot() +
  stat_function(geom = "area",
                fill = "4",
                alpha = .4,
                fun = .pshade, 
                args = list(min = 0, max = f_krit, df1 = df1, df2 = df2),
                n = detail) +
    stat_function(geom = "area",
                fill = "7",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_krit, max = xlim, df1 = df1, df2 = df2),
                n = detail) +
 stat_function(geom = "area",              # p-Wert
                fill = "red",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_emp, max = xlim_new, df1 = df1, df2 = df2),
                n = detail) +  
    geom_function(
            fun = df, args = list(df1 = df1, df2 = df2), n = detail) +
    annotate("segment", x = f_krit, y = 0.3, xend = f_krit, yend = 0) +
    annotate("label", x = f_krit, y = 0.3, label = "kritische\nGrenze") +
    annotate("label", x = mean(c(f_krit, xlim_new)), y = 0.04, label = "α = 5 %") +
    annotate("label", x = 2.1, y = 0.05, label = "1 - α = 95 %") +
    annotate("segment", x = f_emp, y = 0, yend = .3, xend = f_emp) +
    annotate("label", y = 0.3, x = f_emp, label = "empirischer\nF-Wert") + 
    annotate("label", y = .13, x = 3.63, label = "p = 5.21 %") +
    geom_brace(aes(x = c(0, f_krit), y = c(0.0, 0.03))) +
    geom_brace(aes(x = c(f_krit, xlim_new), y = c(0.0, 0.023))) +
    geom_brace(aes(x = c(f_emp, xlim_new), y = c(0.05, 0.115)), mid = 0.29) +
    
    scale_x_continuous(limits = c(0, xlim_new)) +
    coord_cartesian(ylim = c(0, 0.5), xlim = c(0, xlim_new), expand = F) +
    labs(x = "F(1, 29)", y = "Wahrscheinlichkeitsdichte", title = "Beispiel",
         subtitle = "nicht signifikant (p > α)") +
    theme_minimal()

# Fail, does not render correctly in output
# library(pBrackets)
# grid.brackets(518, 453, 858, 453, h = .1)
# grid.brackets(702, 478, 858, 478, h = .09)
# grid.brackets(50, 478, 702, 487, h = .1, curvature = 0.2)


```


### Lokaltest

Der Lokaltest testet jeden Regressionskoeffizienten einzeln, und ist deswegen für die multiple lineare Regression wichtig, wo es mehrere Prädiktoren gibt. 

Im Fall der einfachen linearen Regression entspricht der Lokaltest dem Test für das Gesamtmodell, weil das gesamte Modell nur aus einem Prädiktor besteht.

Das lässt sich auch schön zeigen anhand der Beziehung zwischen der $t$-Statistik aus dem Lokaltest und der $F$-Statistik aus dem Gesamtmodelltest:

Generell gilt: $F = t^2$.

mit $t^2 = 20.48^2 = 419.4 = F$

Wo finde ich diese Werte im R-Output?

#### R-Output

Die Ergebnisse der Lokaltests befinden sich im Abschnitt `Coefficients`:

```{r localtest_output, exercise = T, exercise.eval = TRUE, exercise.setup = "silentsetup"}
fit_centered <- lm(Volume ~ Diameter, data = trees)
summary(fit_centered)
```

Dabei ist wieder vor allem der $p$-Wert relevant. Er befindet sich in der Spalte `Pr(>|t|)`.

Der $p$-Wert für den Test von $b_1$ finden wir in der Zeile `Diameter`, also der Variablenname des Prädiktors.

Er ist identisch mit dem Wert aus dem Gesamtmodelltest! Auch das stützt, dass die Tests im Fall der einfachen linearen Regression äquivalent sind. 

```
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.046122   0.095290  -10.98 7.62e-12 ***
Diameter     0.056476   0.002758   20.48  < 2e-16 *** <----- p-Wert
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

Den Lokaltest für *Intercept* ignorieren wir, da es in der Regel nicht interessant ist zu wissen, ob der Achsenabschnitt in Wirklichkeit 0 ist - das wäre ja eigentlich gar kein Problem, deswegen müssen wir das auch nicht herausfinden. 

Wichtig ist, herauszufinden, ob sich das Regressionsgewicht signifikant von 0 unterscheidet. 

Da der Lokaltest nur für die multiple lineare Regression Bedeutung entfaltet, gehen wir nicht weiter im Detail darauf ein.

Sie haben jetzt hoffentlich erfolgreich verstanden:

- warum wir unser Modell testen
- was wir testen
- wo Sie den $p$-Wert finden, sowohl für Lokaltest als auch Gesamtmodelltest

::: aufgabe
Dazu gibt es jetzt noch ein paar Übungsaufgaben mit anderen R-Outputs!
:::

```{r}

```


## Visualisierungen

Es gibt viele Wege, eine Punktwolke mit Regressionsgerade in R herzustellen.

Ich stelle hier nur zwei vor:

1.  Der schnelle Weg, mit base R
2.  Der schöne Weg, mit ggplot

Aber es ist unfassbar wichtig, mindestens einen dieser Wege zu können, um schnell und unkompliziert visuell zu überprüfen, ob die Daten überhaupt einen linearen Zusammenhang haben, und auch um zu sehen, wie gut die Gerade die Datenpunkte "erklärt".

### Der schnelle Weg mit base R

```{r, echo = TRUE}
fit <- lm(Volume ~ Diameter, data = trees) # Modell erstellen

plot(Volume ~ Diameter, data = trees) # Punktdiagramm erstellen

abline(fit) # Regressionsgerade ins Punktdiagramm einbinden
```

#### Code Breakdown

-   Modell erstellen mit `lm()` und abspeichern wie gewohnt.

-   `plot()`:

    Funktioniert genau wie `lm` mit der Formelschreibweise `y ~ x`

    (Pluspunkt für Konsistenz)

-   `abline()`:

    heißt literally $a$-$b$-Line und fügt eine Gerade zum Plot hinzu.

    $a$ ist in dem Fall Intercept und $b$ Slope.

    Das aufregende und schöne ist: Wir können einfach das gesamte gespeicherte Modell `fit` als Argument übergeben, und `abline()` weiß selbst wo die Koeffizienten zu finden sind und liest sie aus.

#### Übung

::: aufgabe
Erstellen Sie selbst ein simples Punktdiagramm mitsamt Regressionsgerade mit base R.

Damit es etwas spannender wird, verwenden wir mal einen anderen Datensatz: In `airquality` finden Sie Daten der Luftqualität in New York im Sommer 1973. Unter anderem gemessen wurde die Windgeschwindigkeit in Meilen pro Stunde, und die maximale Tagestemperatur in Grad Fahrenheit.

1.  Stellen Sie ein Regressionsmodell namens `temp_model` auf, was Veränderungen der Temperatur durch Veränderungen der Windgeschwindigkeit erklären kann.
2.  Erstellen Sie ein Punktdiagramm
3.  Fügen Sie die Regressionsgerade hinzu

Variablennamen: `Wind` = Windgeschwindigkeit `Temp` = Temperatur
:::

```{r slopeestim}
question_radio("Geben Sie einen Tipp ab: Welche Steigung wird die Regressionsgerade vermutlich haben?",
               answer("positiv", message = "Leider nicht so wahrscheinlich - dann würde es heißer werden, wenn der Wind schneller weht."),
               answer("negativ", correct = T, message = "Sinnvoll, da höhere Windgeschwindigkeit eher zu niedrigeren Temperaturen führt -> negativer Zusammenhang"))
```

```{r baser, exercise = TRUE, exercise.setup = "silentsetup", exercise.cap = "base R Grafik"}

```

```{r baser-solution}
temp_model <- lm(Temp ~ Wind, data = airquality) # Modell erstellen

plot(Temp ~ Wind, data = airquality) # Punktdiagramm erstellen

abline(temp_model) # Regressionsgerade zum Plot hinzufügen
```

### Der schöne Weg mit ggplot

```{r, echo = T}
library(ggplot2) # Paket laden

ggplot(trees, aes(x = Diameter, y = Volume)) +   # Daten und Mappings definieren
  geom_point() +                                 # Punktdiagramm
  geom_smooth(method = "lm", se = F) +           # Regressionsgerade
  theme_minimal() +                              # Schnick-Schnack
  labs(x = "Durchmesser (cm)", y = "Volumen (m³)", title = "Amerikanische Traubenkirsche")
```

#### Code Breakdown

Ich setze hier Grundkenntnisse in ggplot voraus, da das den Rahmen sprengen würde (siehe Tutorial zur Visualisierung).

-   `geom_smooth()` ist das Geom, was wir verwenden um eine Regressionsgerade zu zeichnen. Der entscheidende Unterschied zu base R ist, dass wir kein Modell aufstellen müssen vorher, sondern das wird automatisch von `geom_smooth()` intern übernommen.

    -   `method = "lm"`: Da es auch viele andere mögliche Modelle gibt, müssen wir im Argument `method` angeben, dass wir die Funktion `lm` nutzen wollen, also ein **l**ineares **M**odell.

        Intern wird dadurch `lm()` aufgerufen, standardmäßig mit der Formel `y ~ x`. Das Ergebnis hängt davon ab, welche Variablen wir auf $x$ mappen und auf $y$. Dieser Sachverhalt wird uns auch in einer neutralen Nachricht bewusst gemacht: `geom_smooth() using formula = 'y ~ x'`.

    -   `se = F`: **s**tandard **e**rror = **F**ALSE. Standardmäßig ist dieses Argument `TRUE`, und es wird automatisch ein Konfidenzintervall mit eingezeichnet, was wir aber nicht benötigen im Moment.

#### Übung

::: aufgabe
Erstellen Sie nun analog zur letzten Übung eine Grafik mittels ggplot, die den Einfluss von Wind auf die Temperatur in einem Punktdiagramm mit Regressionsgerade darstellt.

Die Daten befinden sich wieder im `airquality`-Datensatz.
:::

```{r airggplot, exercise = TRUE, exercise.cap = "ggplot Grafik"}

```

```{r airggplot-solution}
library(ggplot2)

ggplot(airquality, aes(x = Wind, y = Temp)) +  # Datensatz und Mappings definieren
  geom_point() +                          # Punktdiagramm zeichnen
  geom_smooth(method = "lm", se = F) +    # Regressionsgerade zeichnen
  theme_minimal()                         # weißer Hintergrund

# Zusatzaufgabe:
                     
#  geom_smooth(method = "lm", se = F, color = "red") + 
```

::: aufgabe
**\* Zusatzaufgabe**

Ändern sie die Farbe der Regressionsgeraden!
:::

Super, jetzt haben Sie gelernt, wie eine einfache lineare Regression visuell dargestellt werden kann!

Im nächsten Kapitel werden wir uns mit den Vorannahmen beschäftigen, die erfüllt sein müssen, damit die lineare Regression überhaupt zulässig ist.

## Vorraussetzungen prüfen

Hier ein kleiner Überblick, darüber, wie die Voraussetzungen geprüft werden können in R:

+----------------------------------------+-----------------------------------+
| Vorannahme                             | Überprüfung                       |
+========================================+===================================+
| 1.  metrische Variablen                | Vorwissen über die Erhebung       |
+----------------------------------------+-----------------------------------+
| 2.  Zufällige Stichprobe               | Vorwissen über die Erhebung       |
+----------------------------------------+-----------------------------------+
| 3.  Linearer Zusammenhang in den Daten | Streudiagramm                     |
+----------------------------------------+-----------------------------------+
| 4.  Es gibt Variation beim Prädiktor   | Streudiagramm                     |
+----------------------------------------+-----------------------------------+

: Vor dem Modellieren

+----------------------------------------------+-----------------------------------------------+
| Vorannahme                                   | Überprüfung                                   |
+==============================================+===============================================+
| 5.  Unkorreliertheit der Residuen mit der UV | `plot(fit)`                                   |
+----------------------------------------------+-----------------------------------------------+
| 6.  Homoskedastizität                        | `plot(fit)`                                   |
+----------------------------------------------+-----------------------------------------------+
| 7.  keine Autokorrelation der Residuen       | `car::durbinWatsonTest()`                     |
+----------------------------------------------+-----------------------------------------------+
| 8.  Normalverteilung der Residuen            | -   Q-Q-Plot: der zweite Plot bei `plot(fit)` |
|                                              |                                               |
|                                              | -   Histogramm der standardisierten Residuen  |
|                                              |                                               |
|                                              |     -   `library(tidyverse)`                  |
|                                              |                                               |
|                                              |     -   `resid(fit) %>% scale() %>% hist()`   |
+----------------------------------------------+-----------------------------------------------+

: Nach dem Modellieren

### 1. Metrische Variablen

Sowohl der Prädiktor als auch das Kriterium müssen metrisch sein.

Hier sind einige andere Verfahren bei:

kategorialer Prädiktor + metrisches Kriterium

-   bei zwei Kategorien: $t$-Test

-   bei mehr als zwei Kategorien: Varianzanalyse (ANOVA) oder Dummy-Kodierung

metrischer Prädiktor + kategoriales Kriterium:

-   logistische Regression

### 2. Zufällige Stichprobe

Im Beispiel mit der Amerikanischen Traubenkirsche wäre es zum Beispiel wichtig, dass nicht selektiv nur Bäume vermessen wurden, die eine besonders gute Wuchsform hatten, sondern eine zufällige Auswahl aus allen möglichen Bäumen getroffen wurde - sonst ist das Modell nicht generalisierbar.

### 3. Linearer Zusammenhang in den Daten

Manche Daten werden besser nicht durch Geraden erklärt, sondern haben vielleicht quadratische Zusammenhänge, wie in diesem Beispiel:

```{r}
data <- data.frame(Temperatur=c(6, 9, 12, 14, 30, 35, 40, 47, 51, 55, 60),
                   Enzymaktivität=c(14, 28, 50, 70, 89, 94, 90, 75, 59, 44, 27))


straight_fit <- lm(Enzymaktivität ~ Temperatur, data = data)
data$Temperatur2 <- data$Temperatur^2
quad_fit <- lm(Enzymaktivität ~ Temperatur + Temperatur2, data = data)
zeit <- seq(0, 60, 0.1)
prediction <- predict(quad_fit, list(Temperatur = zeit, Temperatur2 = zeit^2))

plot(Enzymaktivität ~ Temperatur, data = data)
abline(straight_fit, col = "red")
lines(zeit, prediction, col = "blue")
```

Die Gerade erklärt die Daten nicht gut und trifft falsche Vorhersagen, während die quadratische Funktion sehr nah an der Realität liegt. (Fiktive Daten, Quelle: [statology](https://www.statology.org/quadratic-regression-r/))

Die Überprüfung einer linearen Beziehung in den Daten erfolgt visuell über ein Punktdiagramm. Die Punkte sollten sich alle durch eine Gerade beschreiben lassen und keine systematischen Abweichungen haben.

Beispiele:

```{r plotmaking, fig.height=5, fig.width=6}
p1 <- ggplot(rtutorials::concrete, aes(x = coarse_aggregate, y = compressive_strength)) +
  geom_point(alpha = 0.2) +
  theme_void() +
  labs(title = "1")

p2 <- ggplot(rtutorials::heated, aes(x = edvisits, y = maxtemp)) +
  geom_point(alpha = 0.18) +
  theme_void() +
  labs(title = "2")

p3 <- ggplot(rtutorials::airfoil, aes(x = angle, y = displacement_thickness)) +
  geom_point(alpha = 0.03) +
  theme_void() +
  labs(title = "3")

p4 <- ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +
  geom_point(alpha = 0.2) +
  theme_void() +
  labs(title = "4")

p5 <- ggplot(women, aes(x = height, y = weight)) +
  geom_point(alpha = 0.5) +
  theme_void() +
  labs(title = "5")
  
p6 <- ggplot(faithful, aes(x = eruptions, y = waiting)) +
  geom_point(alpha = 0.34) +
  theme_void() +
  labs(title = "6")

gridExtra::grid.arrange(p1, p2, p3, p4, p5, p6)
```

```{r plotquiz}
question_checkbox("Bei welchen dieser Plots würden Sie eine zugrunde liegende lineare Beziehung vermuten?",
                  answer("1", message = "Hier ist es schwer zu sagen - die Daten enthalten sehr viel Streuung."),
                  answer("2", message = "Diese Beziehung zwischen Notaufnahmebesuchen (y) und Hitze (x) folgt eher einer Kurve als einer Gerade"),
                  answer("3", message = "Hier liegt scheinbar eher eine Exponentialfunktion zu Grunde, keine Gerade"),
                  answer("4", correct = T, message = "4: Hier könnte eine Gerade die Punkte gut beschreiben."),
                  answer("5", correct = T, message = "5: Es handelt sich um den Zusammenhang zwischen Körpergröße und Gewicht."),
                  answer("6", correct = T, message = "6: Auch hier lässt sich eine lineare Beziehung vermuten."),
                  allow_retry = TRUE
         )
```

### 4. Variation des Prädiktors

Wie sieht ein Plot ohne Variation des Prädiktors aus?

Alle Punkte liegen auf dem selben $x$-Wert. In diesem Fall hilft $x$ überhaupt nicht, Varianz in $y$ zu erklären. Die beste Vorhersage für $y$ ist dann der Mittelwert $\bar y$.

Dieser Fall ist in der Praxis extrem unwahrscheinlich, da es meistens ein bisschen zufällige Variation gibt.

```{r novariation, fig.height=5, fig.width=5}
plot(x = rep(20, 100), y = rnorm(100), yaxt = "n", xaxt = "n", xlab = "x", ylab = "y")
```

::: aufgabe
**\* Profi-Frage**
:::

```{r proquestion}
question_numeric("Welchen Wert würde $R^2$ im obigen Plot annehmen und warum?",
                 answer(0, correct = T),
                 allow_retry = T)
```

<details>
<summary>**▼ Erklärung**</summary>

::: infobox
\begin{align}
R^2 &= \frac{\text{Durch das Modell erklärte Varianz in y}}{\text{Gesamtvarianz in y}} \\
R^2 &= \frac{0}{\text{Gesamtvarianz in y}}
R^2 &= 0
\end{align}
:::
</details>

### 5. Unkorreliertheit der Residuen mit Kriterium

### 6. Homoskedastizität

### 7. keine Auto-Korrelation der Residuen

### 8. Normalverteilung der Residuen

## Ausblick: Multiple lineare Regression

In der Praxis wird selten nur eine einfache (im Sinne von 1-fach) lineare Regression gerechnet, sondern mehrere Prädiktoren werden verwendet, um ein Kriterium vorherzusagen oder zu erklären.

Bei unserem Forst-Beispiel ist das ja auch so: Die Höhe spielt natürlich eine wichtige Rolle, und ist nicht zu vernachlässigen, wenn wir einen guten Schätzer für das Holzvolumen eines Baumes haben wollen.

```{r}
trees$Height <- trees$Height * 0.3048  # Umrechnung Fuß in Meter

multiple_fit <- lm(Volume ~ Diameter + Height, data = trees)
summary(multiple_fit)
anova(fit, multiple_fit)
```

## Abschlussquiz

## Learnings
